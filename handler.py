try:
    import unzip_requirements
except ImportError:
    pass
import torchvision.transforms as transforms
from PIL import Image

import boto3
import os
import io
import json
import base64
from requests_toolbelt.multipart import decoder
# import utils
# from models.pose_resnet import get_pose_net
# from core.config import config
# from core.config import update_config
import cv2
import re
import onnx
import onnxruntime
import numpy as np

print("Import End...")

S3_BUCKET   =   os.environ['S3_BUCKET'] 
QUANTIZED_MODEL_PATH = os.environ['QUANTIZED_MODEL_PATH']
#CONFIG_FILE_DIR = os.environ['CONFIG_FILE_DIR']

POSE_PAIRS = [
# UPPER BODY
              [9, 8],
              [8, 7],
              [7, 6],

# LOWER BODY
              [6, 2],
              [2, 1],
              [1, 0],

              [6, 3],
              [3, 4],
              [4, 5],

# ARMS
              [7, 12],
              [12, 11],
              [11, 10],

              [7, 13],
              [13, 14],
              [14, 15]
]

JOINTS = ['0 - r ankle', '1 - r knee', '2 - r hip', '3 - l hip', '4 - l knee', '5 - l ankle', '6 - pelvis', '7 - thorax', '8 - upper neck', '9 - head top', '10 - r wrist', '11 - r elbow', '12 - r shoulder', '13 - l shoulder', '14 - l elbow', '15 - l wrist']
JOINTS = [re.sub(r'[0-9]+|-', '', joint).strip().replace(' ', '-') for joint in JOINTS]

print('Downloading model...')

s3 = boto3.client('s3')

try:
    if os.path.isfile(MODEL_PATH) != True:
        obj = s3.get_object(Bucket=S3_BUCKET, Key=QUANTIZED_MODEL_PATH)
        print("Creating Bytestream")
        bytestream = io.BytesIO(obj['Body'].read())
        print("Loading config")
        #update_config(CONFIG_FILE_DIR)
        print('Loading Model')
        ort_session = onnxruntime.InferenceSession("quantized_model_1.onnx")
        #model = get_pose_net(config, is_train=False)
        #model.load_state_dict(torch.jit.load(bytestream))
        print('Model Loaded...')
except Exception as e:
    print(repr(e))
    raise(e)

get_keypoints = lambda pose_layers: map(itemgetter(1, 3), [cv2.minMaxLoc(pose_layer) for pose_layer in pose_layers])

def transform_image(image_bytes):
    try:
        transformations = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])
        image = Image.open(io.BytesIO(image_bytes))
        return transformations(image).unsqueeze(0)
    except Exception as e:
        print(repr(e))
        raise(e)


def to_numpy(tensor):
    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()


def get_hpe(image_bytes):
    tensor = transform_image(image_bytes=image_bytes)
    # with torch.no_grad():
    #     result = model.forward(tensor)
    # result = np.array(res.detach().squeeze())
    # compute ONNX Runtime output prediction
    ort_inputs = {ort_session.get_inputs()[0].name: np.squeeze(to_numpy(x.unsqueeze(0)), axis=0)}
    print("Shape of the output of ONNX Model ",np.squeeze(to_numpy(x.unsqueeze(0)), axis=0).shape)
    ort_outs = ort_session.run(None, ort_inputs)
    return result

def hpe(event, context):
    try:
        content_type_header = event['headers']['content-type']
        body = base64.b64decode(event['body'])
        print('Loading body of the event...')

        picture = decoder.MultipartDecoder(body, content_type_header).parts[0]
        result = get_hpe(image_bytes=picture.content)
        print("points with hpe is generated by the model")
        image_hpe = get_image_with_points_connected_lines(picture.content, result)
        image_hpe = Image.fromarray(cv2.cvtColor(image_hpe, cv2.COLOR_RGB2BGR))
        filename = picture.headers[b'content-Disposition'].decode().split(';')[1].split('=')[1]
        if len(filename) < 4:
            filename = picture.headers[b'content-Disposition'].decode().split(';')[2].split('=')[1]

        return {
            "statusCode": 200,
            "headers": {
                'content-type': 'application/json',
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Credentials': True

            },
            "body": json.dumps({'file':filename.replace('"',''), 'fileContent':base64.b64encode(image_hpe).decode("utf-8")})
        }
    except Exception as e:
        print(repr(e))
        return {
            'statusCode': 500,
            "headers": {
                'content-type': 'application/json',
                'Access-Control-Allow-Origin': '*',
                "Access-Control-Allow-Credentials": True
            },
            "body": json.dumps({"error: repr(e"})
        }


def draw_points_on_image(img, result):
    points_list = []
    image = cv2.resize(img, OUTPUT_IMAGE_SHAPE)
    w, h = OUTPUT_IMAGE_SHAPE
    # print(image.shape)
    r_w, r_h = result.shape[1:]
    for mat in result:
        x, y = getpoint(mat)
        # print(x, y)
        t_x = w/r_w*x
        t_y = h/r_h*y
        t_x, t_y = int(t_x), int(t_y)
        points_list.append((t_x, t_y))
        # print("Transformed : [{} {}]".format(t_x, t_y))
        cv2.circle(image, (t_x, t_y), 2, (255, 255, 255), 2)
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    return image



def get_image_with_points_connected_lines(img, result):
    threshold = 0.6
    res_height, res_width = 64, 64
    out_shape = (res_height, res_width)
    image_p = cv2.imread(img)
    pose_layers = result
    key_points = list(get_keypoints(pose_layers=pose_layers))
    is_joint_plotted = [False for i in range(len(JOINTS))]
    for pose_pair in POSE_PAIRS:
        from_j, to_j = pose_pair

        from_thr, (from_x_j, from_y_j) = key_points[from_j]
        to_thr, (to_x_j, to_y_j) = key_points[to_j]

        img_height, imh_width, _ = image_p.shape

        from_x_j, to_x_j = from_x_j * imh_width / out_shape[0], to_x_j * imh_width / out_shape[0]
        from_y_j, to_y_j = from_y_j * img_height / out_shape[1], to_y_j * img_height / out_shape[1]

        from_x_j, to_x_j = int(from_x_j), int(to_x_j)
        from_y_j, to_y_j = int(from_y_j), int(to_y_j)

        if from_thr > threshold and not is_joint_plotted[from_j]:
            # this is a joint
            cv2.ellipse(image_p, (from_x_j, from_y_j), (4, 4), 0, 0, 360, (255, 255, 255), cv2.FILLED)
            is_joint_plotted[from_j] = True

        if to_thr > threshold and not is_joint_plotted[to_j]:
            # this is a joint
            cv2.ellipse(image_p, (to_x_j, to_y_j), (4, 4), 0, 0, 360, (255, 255, 255), cv2.FILLED)
            is_joint_plotted[to_j] = True

        if from_thr > threshold and to_thr > threshold:
            # this is a joint connection, plot a line
            cv2.line(image_p, (from_x_j, from_y_j), (to_x_j, to_y_j), (255, 74, 0), 2)
    image_p = cv2.cvtColor(image_p, cv2.COLOR_RGB2BGR)
    return image_p

